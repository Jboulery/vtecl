<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>VTECL - Le DeepLearning appliqué au FaceSwap</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Le DeepLearning appliqué au FaceSwap</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">A propos</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#intro">Introduction</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#fakeapp">L'Application FakeApp</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#ethic">Du point de vue éthique</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#awards">Perspectives</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Le DeepLearning appliqué au FaceSwap - 
            <span class="text-primary">FakeApp</span>
          </h1>
          <div class="subheading mb-5">Veille technologique menée par Jérémy Boulery - Ecole Centrale de Lyon : 
            <a href="mailto:jeremy.boulery@ecl14.ec-lyon.fr">jeremy.boulery@ecl14.ec-lyon.fr</a>
          </div>
          <p class="mb-5">Cette étude est menée dans le cadre du MOS 4.4 - Nouvelles Technologies de l'Information et de la Communication. Elle concerne l'application du DeepLearning à la création de fake vidéos et ses implications en termes de perspectives et d'éthique. L'application FakeApp dont la première version date de Décembre 2017 est le principal support de ce travail de par l'engouement qu'elle a rapidement suscité.</p>
          <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
              <a href="https://twitter.com/Darshut">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/j%C3%A9r%C3%A9my-boulery-0574b295/">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/Jboulery">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="intro">
        <div class="my-auto">
          <h2 class="mb-5">Introduction</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Pourquoi ce sujet spécifique ?</h3>
              <div class="subheading mb-3">Récent et très controverse</div>
              <p>A l'heure de présenter les résultats issues de cette veille, une réponse à cette question me paraît plus qu'indispensable pour débuter.</p>
              <p>S'attaquer à un sujet spécifique lors d'une veille implique une raréfaction des sources d'information. De fait, la release très récente de l'application FakeApp (Décembre 2017) et ses spécificités techniques impliquent qu'il est vraiment peu surprenant que j'ai été le premier et pendant un bon moment le seul à associer #DeepLearning à #FaceSwap...</p>
              <p>La possibilité donnée par le MOS Nouvelles Technologies de l'Information et de la Communication d'approfondir un sujet en particulier m'a alors paru un bon moyen de passer du temps à essayer de comprendre le fonctionnement et les répercussions attendues par cette release que d'aucuns qualifient tout simplement de "révolution du fake". Mon attention s'est portée sur FakeApp pour deux principales raisons :
              	<ul>
              		<li>La croissance rapide du nombre d'utilisateurs et de la communauté autour du projet originellement nommé d'après son créateur "deepfakes"</li>
              		<li>La controverse immédiate sur les utilisations malignes dans lesquelles FakeApp trouve son origine, à savoir plus précisément la création de fake contenu à caractère pornographique.</li>
              	</ul>
              	Ces deux points sont illustrés ci-dessous par des chiffres datant de seulement quelques semaines après la release:
              </p>
              	<div class="row justify-content-center">
              		<div style="margin-bottom: 10px;">
		          		<img class="img-responsive" src="img/github_stars.png" alt="Intérêt parmi les développeurs"/>
		          		<figcaption>3.5k Stars sur Github courant Janvier 2018</figcaption>
		          	</div>
		          	<div>
		          		<img class="img-responsive" src="img/fakeapp_downloads.png" alt="Intérêt parmi les utilisateurs"/>
		          		<figcaption>100k+ Téléchargements de FakeApp 3 semaines après la release</figcaption>
		          	</div>
              	</div>
            </div>
            <div class="resume-date text-md-right">
              <img id="tweets_darshut_img" src="img/tweets_darshut.png" alt="Tweets relatifs à #DeepLearning et #FaceSwap" />
              <figcaption>Petit sentiment de solitude...</figcaption>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Introduction rapide aux réseaux de neurones</h3>
              <div class="subheading mb-3">Base du fonctionnement de l'application</div>
              <p>Cette partie n'a pas vocation à faire comprendre au lecteur le fonctionnement en détail d'un réseau de neurones car ce n'est pas le coeur du sujet ici traité. Vous trouverez donc ci-dessous simplement des éléments de compréhension sur l'application des réseaux de neurones au traitement d'images tel que le fait FakeApp. Pour les curieux, vous pourrez trouver les travaux de mes collègues François Paugam et Alexandre Popov très bien fournis et détaillés à ce propos:
					<ul>
						<li>Alexandre Popov : <a href="https://docs.google.com/presentation/d/1Rc-2TPLePmazuuiW65CXXnXTDpDW22TTT_gjK90tvVI/edit?usp=sharing">Deep Learning appliqué au traitement d'images</a> - Site internet : En construction - Twitter : <a href="https://twitter.com/APop41670853">@APop41670853</a></li>
						<li>François Paugam : <a href="https://docs.google.com/presentation/d/16GSVmvPO-55EcUWf4OQXvmp3SWb8YHenJNAFfIgYdZs/edit?usp=sharing">Generative Adversarial Networks, quand les machines imaginent</a> - Site internet : <a href="https://francoispgm.github.io/">https://francoispgm.github.io/</a> - Twitter : <a href="https://twitter.com/francois_vtecl">@francois_vtecl</a></li>
					</ul>
				Au cours de ma veille, j'ai également eu l'occasion de visionner plusieurs vidéos extrêmement bien faites dont les deux suivantes réalisées par 3Blue1Brown :
				</p>
				<div class="row justify-content-center" style="margin: 10px;">
					<figure>
						<iframe width="640" height="420" src="https://www.youtube.com/embed/aircAruvnKk"></iframe>
						<figcaption>But what *is* a Neural Network? | Chapter 1, deep learning - 3Blue1Brown</figcaption>
					</figure>
				</div>
				<div class="row justify-content-center" style="margin: 10px;">
					<figure>
						<iframe width="640" height="420" src="https://www.youtube.com/embed/IHZwWFHWa-w"></iframe>
						<figcaption>Gradient descent, how neural networks learn | Chapter 2, deep learning - 3Blue1Brown</figcaption>
					</figure>
				</div> 
				<p>Dans la suite, les images présentées sont extraites du cours de Fei-fei LI, Andrej Karpathy et Justin Johson.<br>
				Un réseau de neurone présente de manière générale :
				<ul>
					<li>Une couche d'entrée : Récupère les paramètres d'entrées (informations sur les pixels d'une image par exemple)</li>
					<li>Une ou plusieurs couches "cachées" : Effectue des calculs sur les éléments donnés en entrée</li>
					<li>Une couche de sortie : Renvoie une classe (ou label) sensée caractérisée les paramètres d'entrée.</li>
				</ul>
				<div class="row justify-content-center">
					<figure>
						<img src="img/2layers_nn.png" alt="Réseau de neurones à deux couches cachées"/>
						<figcaption>Réseau de neurones à deux couches cachées</figcaption>
					</figure>
				</div>
				Prenons un cas concret qui est certainement le "Hello World!" des réseaux de neurones : la reconnaissance de chiffres manuscrits.
				Nous avons au départ, une base de données d'images représentant des chiffres manuscrits. Ces images sont associées à un label qui n'est rien d'autre que la valeur du chiffre de l'image.
				<div class="row justify-content-center">
					<figure>
						<img src="img/hand_written_digits_db.png" alt="Extrait de la base de données d'images de chiffres manuscrits à disposition">
						<figcaption>Extrait de la base de données d'images de chiffres manuscrits à disposition</figcaption>
					</figure>
				</div>
				<div class="row justify-content-center">
					<figure>
						<img src="img/labels.png" alt="Association de chaque image à un label">
						<figcaption>Association de chaque image à un label</figcaption>
					</figure>
				</div>
				Le but du jeu est donc, comme vous l'aurez deviné, de faire en sorte que la machine arrive à associer le bon label à chaque image (sans qu'on ne lui fournisse le label évidemment...).
				Une première étape consiste à scinder la base de données en deux :
				<ul>
					<li>La première partie sera utilisée pour entraîner le réseau (i.e. construire le modèle mathématique des couches cachées) : on parle de données d'entraînement</li>
					<li>La seconde partie sera utilisée pour tester si le réseau de neurones arrive à prédire correctement de quel chiffre il s'agit : on parle de données de test</li>
				</ul>
				On peut imaginer une division du type 80% des données seront utilisées pour l'entraînement et 20% pour le test.
				</p>
				<p>une fois cette division effectuée, on choisit (de manière arbitraire disons le) une architecture pour le réseau. A savoir que plus nombreux seront les couches et les neurones, plus lourds seront les calculs à faire. On peut tout de même faire de l'heuristique pour déterminer un nombre adéquat de couches (cf première vidéo de 3Blue1Brown).</p>
				<p>L'étape d'entraînement consiste à fournir les images des données d'entraînement en entrée du réseau. Chaque image est envoyée à la couche d'entrée du réseau de neurones de manière que chaque noeud (=neurone) de la couche d'entrée contient les informations de niveaux de gris d'un pixel de l'image donnée. Ces niveaux de gris sont alors envoyés aux couches cachées du réseau qui vont affecter à chacun des pondérations suivant un modèle linéaire. Elles renvoient après calculs les poids obtenus à la couche de sortie qui en déduit la réponse à afficher.
				Cette réponse est alors comparée au label correspondant à l'image et si erreur il y a, on renvoie celle-ci vers le début du réseau (cf fonction loss et backward propagation) afin d'effectuer les modifications sur les calculs à exécuter.
				</p>
				<p>Lorsque les résultats sur les données d'entraînement sont satisfaisants, on demande au réseau de neurones d'effectuer des prédictions sur le jeu de test. Il n'aura alors plus la possibilité de comparer son erreur en sortie. Ces données lui étant inconnues, la précision et le "recall" obtenus feront état de la performance de l'algorithme.</p>
				<div class="subheading mb-3">Spécificité de FakeApp</div>
				<p>L'application FakeApp est basée sur des réseaux de neurones dits de convolution et de déconvolution. Pour en savoir plus, référez-vous aux ressources ci-dessus. Pour faire simple, le réseau de convolution (ou encoder) est chargé d'extraire les caractéristiques des visages pour chaque personne. Le réseau de déconvolution (ou decoder) va quant à lui essayer de reconstruire le visage de la personne à partir des caractéristiques extraites par l'encoder. L'algorithme sera considéré comme suffisamment entraîné lorsque le decoder parviendra à reconstituer de manière satisfaisante le visage de la personne.</p>
				<p>A savoir que FakeApp se base sur TensorFlow, outil OpenSource de Machine Learning développé par Google.</p>
            </div>
          </div>

        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="fakeapp">
        <div class="my-auto">
          <h2 class="mb-5">FakeApp</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Quelques exemples pour commencer</h3>
              <div class="subheading mb-3">Nicolas Cage - Star du fake</div>
              <p>Avant de rentrer dans le vif du sujet, une petite mise en bouche me paraît faire sens. Voici donc quelques réalisations obtenues à partir de l'utilisation de FakeApp, application destinée à la création de fake vidéos. Il faut savoir que Nicolas Cage est rapidement devenu la célébrité dont le visage a été le plus utilisé dans la création de fakes. Ces fakes sont à vocation parodiques et leur étrangeté autant que leur réalisme prêtent effectivement à rire.</p>
              <div class="row justify-content-center">
              	<figure style="margin: 10px">
              		<iframe width="640" height="420" src="https://www.youtube.com/embed/UwiagqaX4fA"></iframe>
					<figcaption>Superman and lois lane deep fake nicolas cage meme - Rage</figcaption>
              	</figure>
				<figure style="margin: 10px;">
					<iframe width="640" height="420" src="https://www.youtube.com/embed/dh-QM54RuAs"></iframe>
					<figcaption>Cage will survive - Vaya Sinola</figcaption>
				</figure>
              </div>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Fonctionnement de l'application</h3>
              <div class="subheading mb-3">Etape 0 - Rassembler des images des deux personnes dont les visages sont à intervertir</div>
              <p>Cela paraît évident mais il est naturel de le préciser. L'étape préalable à l'utilisation de l'application est de constituer deux datasets d'images des deux personnes que nous appellerons pour la suite personne A et personne B. Après retour d'expérience, il semble que quelques centaines d'images soient nécessaires afin d'obtenir un bon résultat. Plus les images sont variées en termes de luminosité, exposition et expressions du visage et plus l'algorithme apprendra efficacement.</p>
              <p>Une stratégie classique afin d'obtenir un grand nombre d'images rapidement est de scinder des vidéos frame par frame. Au rythme de quelques images par seconde, il est aisé d'atteindre les centaines d'images nécessaires.</p>
              <p>Pour la suite, j'ai souhaité pour ma part effectuer le remplacement du visage de Vincent Cassel lors d'une interview par celui de Franck Debouck, directeur de l'Ecole Centrale de Lyon. La vidéo obtenue en fin de compte ne sera pas postée sur YouTube par respect pour le droit à l'image de ces deux personnes publiques mais vous pouvez m'en faire la demande par mail. Les images présentées ci-dessous sont librement accessibles sur internet (YouTube).</p>
              	<div class="row justify-content-center">
					<figure>
						<img src="img/debouck_dataset.jpg" alt="Extrait du dataset constitué pour Franck Debouck">
						<figcaption>Extrait du dataset constitué pour Franck Debouck (ces images sont extraites d'une vidéo)</figcaption>
					</figure>
				</div>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <div class="subheading mb-3">Etape 1 - Extraction des visages des images du dataset</div>
              <p>Chaque image du dataset contient son lot d'éléments pouvant perturber l'étape d'apprentissage de l'algorithme. Pour comprendre, imaginons que l'on ait constituer une base de données d'image pour la personne A à partir d'une vidéo. Imaginons également que sur cette vidéo, il y a un arbre en arrière-plan qui apparaît sur chaque frame. Si on laisse les choses dans cet état, l'algorithme va naturellement penser que l'arbre est un élément constitutif de la personne dont il veut extraire les caractéristiques. Cela impliquera que cet arbre sera considérer lors de l'étape de reconstruction du visage et sera donc très nuisible...</p>
              <p>Il convient donc de resserrer l'image autour du visage de la personne en annulant l'angle de prise de vue de sorte que l'axe des yeux de la personne soit horizontal. Cette étape est entièrement automatisée mise à part vérification du résultat.</p>
              <p>Pour réaliser cela, FakeApp se base sur OpenCV et en particulier sur la fonction "Histogram Of Oriented Gradients" qui permet de repérer un visage dans une image en regardant le flux de lumière dans cette image. Un pattern générique a en effet été calculé à partir de millier de photos de visages. Si un pattern semblable est détecté, on peut considérer qu'il s'agit bien de la face d'une personne.</p>
              	<div class="row justify-content-center">
					<figure>
						<img src="img/face_pattern.png" alt="Pattern générique utilisé par OpenCV">
						<figcaption>Pattern générique utilisé par OpenCV</figcaption>
					</figure>
				</div>
				<p>FakeApp va donc passer en revue toutes les images du dataset et extraire de chacune d'entre elles une image centrée et éventuellement tournée du visage de la personne ciblée. Deux formats sont supportées par la version 1.11 de l'application : png et jpg. Le paramètre "Mult faces" qui permet de repérer plusieurs visages sur une image lorsqu'il vaut "true" est pour le moment assez peu concluant et on préfèrera utiliser des images où la personne est seule.</p>
            	<div class="row justify-content-center">
					<figure>
						<img src="img/debouck_extract.png" alt="Phase d'extraction du visage">
						<figcaption>Phase d'extraction du visage des images du dataset</figcaption>
					</figure>
				</div>
            <p>Cette étape est également réalisée sur la personne B qui dans le cadre de notre exemple sera Vincent Cassel.</p>
            </div>
            <div class="resume-date text-md-right">
              <img src="img/extract_tab.png" alt="L'onglet d'extraction de visage de FakeApp" />
              <figcaption>Fonctionnalité d'extraction de visages</figcaption>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <div class="subheading mb-3">Etape 2 - Entraînement du modèle</div>
              <p>Une fois les visages des deux personnes à intervertir extraites, le modèle peut être entraîné. Dans le cadre d'une transformation A vers B, on souhaite que l'algorithme soit capable de reconstruire le visage de la personne B à partir des caractéristiques du visage de la personne A sur une certaine image.</p>
              <p>Comme mentionné dans la partie expliquant le fonctionnement d'un réseau de neurone et des spécificités de FakeApp, l'application se sert en fait de deux réseaux de neurones travaillant en parallèle.</p>
              	<div class="row justify-content-center">
					<figure>
						<img src="img/training_step.png" alt="Schéma de fonctionnement de l'étape d'entraînement">
						<figcaption>Schéma de fonctionnement de l'étape d'entraînement</figcaption>
					</figure>
				</div>
				<p>Les encoders sont chargés, pour chaque image du dataset, d'extraire les caractéristiques du visage de la personne (d'où l'importance d'avoir diverses expositions, luminosités et expressions pour enrichir le nombre de ces caractéristiques). Ces caractéristiques sont ensuite transmises aux decoders qui vont essayer de reconstruire le visage de la personne à partir de ces caractéristiques. Il est important de noter ici que chaque paire encoder / decoder travaille sur une seule personne uniquement. Il n'est pas encore question d'échanger les visages mais simplement d'apprendre à les reconstruire de manière satisfaisante.</p>
				<p>Le decoder A (resp. B) est donc chargé de reconstruire le visage de la personne A (resp. B) à partir de caractéristiques visages extraites par un encoder. On estime que le decoder A (resp. B) est entraîné si l'erreur commise (au sens de la RMSE) dans la reconstruction d'une image par celui-ci est inférieure à 2% par rapport à l'image donnée en entrée de l'encoder (voire 1% si la résolution est haute).</p>
				<p>Comme vous l'aurez certainement compris, pour invertir les visages de A vers B, il suffit alors de transmettre les caractéristiques extraites par l'encoder A au decoder B qui sait reconstruire le visage de la personne B. Une fonction de prévisualisation permet d'observer le travail des deux réseaux de neurones agissant en parallèle.</p>
				<div class="row justify-content-center">
					<figure>
						<img src="img/cassel_to_debouck.png" alt="Phase d'entraînement du modèle">
						<figcaption>Prévisualisation de l'entraînement du modèle</figcaption>
					</figure>
				</div>
				<p>Les paramètres disponibles dans l'application permettent de régler le nombre de couches cachées et de neurones par couche. Meilleure est la carte graphique à disposition et le plus on peut en mettre. Il semble bien que l'augmentation de couches et de noeuds amène à des résultats sensiblement meilleurs. A titre d'indication voici les réglages pour deux cartes graphiques pour lesquelles on connaît des réglages fonctionnels: 
				<ul>
					<li>NVIDIA GTX960m : 2 couches / 64 neurones par couche</li>
					<li>NVIDIA GTX1080 : 8 couches / 1064 neurones par couche</li>
				</ul>
				D'où l'importance d'avoir un matériel performant, ce qui n'est pas le cas de tout le monde dans le contexte actuel (surconsommation des cartes graphiques par les mineurs de cryptomonnaies) au demeurant. C'est le principal facteur limitant.
				 </p>
            </div>
            <div class="resume-date text-md-right">
              <img src="img/train_tab.png" alt="L'onglet d'entraînement du modèle" />
              <figcaption>Onglet d'entraînement du modèle</figcaption>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <div class="subheading mb-3">Etape 3 - Fusion des visages</div>
              <p>Cette étape est simpliste car elle consiste simplement à coller le visage reconstruit par le decoder B en lieu et place du visage de la personne A. Les informations de position sont par ailleurs extraites dans un fichier JSON <em>alignments.json</em> lors de l'étape 1 d'extraction des visages.</p>
              <p>Une vidéo n'étant qu'une suite d'images, c'est un processus itératif prenant très peu de temps. Il suffit après fusion d'utiliser un logiciel tel que FFMPEG afin de reconstruire une vidéo <em>.mp4</em> à partir d'une suite d'images <em>.png</em>.</p>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="ethic">
        <div class="my-auto">
          <h2 class="mb-5">Du point de vue éthique</h2>
          <p></p>
        </div>
      </section>

    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
